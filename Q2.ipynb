{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementing and Testing the KNN Algorithm on the Iris Dataset\n",
        "\n",
        "In this notebook, we implement the K-Nearest Neighbors (KNN) algorithm from scratch using only the `NumPy` library. Then, we will test, optimize, and evaluate the implemented model on the famous Iris dataset.\n",
        "\n",
        "Workflow Steps:\n",
        "1.  **Section 1:** Implement the `MyKNNClassifier` class in a `knn.py` file.\n",
        "2.  **Section 2:** Load and prepare the Iris dataset (including scaling and splitting the data).\n",
        "3.  **Section 3:** Use `GridSearchCV` and `KFold` (Cross-Validation) to find the best hyperparameters (K, distance metric, and weighting mode).\n",
        "4.  **Section 4:** Final `classification_report` (including Accuracy, Precision, Recall) on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Implementing `MyKNNClassifier`\n",
        "\n",
        "In the cell below, we use the \"magic command\" `%%writefile` to directly save the cell contents into a file named `knn.py`. This file contains the exact class requested in the prompt, with all `fit` and `predict` methods implemented.\n",
        "\n",
        "**Key Implementation Notes:**\n",
        "-   `fit(X, y)`: Simply stores the training data within the object.\n",
        "-   `predict(X)`: Calls a helper function `_predict_one` for each sample in `X` (test set).\n",
        "-   `_predict_one(x)`: This method is the heart of the algorithm:\n",
        "    1.  Calculates the distance from sample `x` to *all* training samples (`self.X_train`) based on the metric (Euclidean or Manhattan). (Fully vectorized with NumPy)\n",
        "    2.  Finds the `k` nearest neighbors using `np.argsort`.\n",
        "    3.  Performs voting based on `weighted=True/False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting knn.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile knn.py\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from collections import Counter\n",
        "\n",
        "class MyKNNClassifier(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    K-Nearest Neighbors Algorithm implementation using NumPy.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_neighbors=3, metric=\"euclidean\", weighted=False):\n",
        "        self.n_neighbors = n_neighbors\n",
        "        self.metric = metric\n",
        "        self.weighted = weighted\n",
        "        self.X_train = None\n",
        "        self.y_train = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Stores the training data.\n",
        "        \"\"\"\n",
        "        # Store X and y in self.X_train and self.y_train.\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        # Return self for sklearn compatibility\n",
        "        return self\n",
        "\n",
        "    def _calculate_distances(self, x):\n",
        "        \"\"\" \n",
        "        Calculates the distance from sample x to all training data.\n",
        "        \"\"\"\n",
        "        if self.metric == \"euclidean\":\n",
        "            # Euclidean distance: sqrt(sum((x_i - y_i)^2))\n",
        "            return np.sqrt(np.sum((self.X_train - x)**2, axis=1))\n",
        "        elif self.metric == \"manhattan\":\n",
        "            # Manhattan distance: sum(|x_i - y_i|)\n",
        "            return np.sum(np.abs(self.X_train - x), axis=1)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown metric: {self.metric}\")\n",
        "\n",
        "    def _predict_one(self, x):\n",
        "        \"\"\"\n",
        "        Predicts the label for a single sample x.\n",
        "        \"\"\"\n",
        "        # 1. Calculate distances\n",
        "        distances = self._calculate_distances(x)\n",
        "        \n",
        "        # 2. Find the k nearest neighbors (indices and distances)\n",
        "        k_indices = np.argsort(distances)[:self.n_neighbors]\n",
        "        k_nearest_labels = self.y_train[k_indices]\n",
        "        k_nearest_distances = distances[k_indices]\n",
        "\n",
        "        # 3. Perform voting\n",
        "        if not self.weighted:\n",
        "            # Unweighted mode: Simple voting (finding the most frequent)\n",
        "            # Use Counter to find the simplest most frequent item\n",
        "            most_common = Counter(k_nearest_labels).most_common(1)\n",
        "            return most_common[0][0]\n",
        "        else:\n",
        "            # Weighted mode: Weighted voting (based on 1 / distance)\n",
        "            # Add a very small value (epsilon) to prevent division by zero\n",
        "            epsilon = 1e-10\n",
        "            weights = 1 / (k_nearest_distances + epsilon)\n",
        "            \n",
        "            # Sum of weights for each class\n",
        "            class_scores = {}\n",
        "            for label, weight in zip(k_nearest_labels, weights):\n",
        "                if label not in class_scores:\n",
        "                    class_scores[label] = 0\n",
        "                class_scores[label] += weight\n",
        "            \n",
        "            # Return the class with the highest score (sum of weights)\n",
        "            return max(class_scores, key=class_scores.get)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts labels for a set of samples X.\n",
        "        \"\"\"\n",
        "        # TODO:\n",
        "        # 1. Call the _predict_one method for each sample in X.\n",
        "        # 2. Return the results as a NumPy array.\n",
        "        if self.X_train is None or self.y_train is None:\n",
        "            raise RuntimeError(\"Model has not been fit() yet.\")\n",
        "            \n",
        "        y_pred = [self._predict_one(x) for x in X]\n",
        "        return np.array(y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Loading and Preparing the Data\n",
        "\n",
        "Now that our class in `knn.py` is ready, we `import` the necessary libraries and the class itself. Then, we load the Iris data, scale it, and split it into training and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MyKNNClassifier class successfully imported from knn.py.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Import the class we wrote\n",
        "try:\n",
        "    from knn import MyKNNClassifier\n",
        "    print(\"MyKNNClassifier class successfully imported from knn.py.\")\n",
        "except ImportError:\n",
        "    print(\"Error: knn.py file not found. Please run the previous cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape (X): (150, 4)\n",
            "Label shape (y): (150,)\n"
          ]
        }
      ],
      "source": [
        "# 1. Load the data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "target_names = iris.target_names\n",
        "\n",
        "print(f\"Data shape (X): {X.shape}\")\n",
        "print(f\"Label shape (y): {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing: Scaling and Splitting\n",
        "\n",
        "The KNN algorithm is distance-based, so if features have different scales (e.g., one from 0 to 1 and another from 0 to 1000), the feature with the larger scale will dominate the distance calculations. \n",
        "\n",
        "**1. Scaling:** We use `StandardScaler` to ensure all features have a mean of 0 and a variance of 1.\n",
        "**2. Splitting:** We divide the data into training (75%) and test (25%) sets. We use `stratify=y` to ensure the class ratios remain the same in both sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 112\n",
            "Number of test samples: 38\n"
          ]
        }
      ],
      "source": [
        "# 1. Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "# 1. Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Number of training samples: {len(X_train_scaled)}\")\n",
        "print(f\"Number of test samples: {len(X_test_scaled)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Hyperparameter Tuning with Cross-Validation\n",
        "\n",
        "To find the best combination of `n_neighbors`, `metric`, and `weighted`, we use `GridSearchCV`. This tool automatically tests all possible combinations using Cross-Validation (here, we define 5-Fold CV) on the **training data**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Grid Search to find the best hyperparameters...\n",
            "Search completed.\n"
          ]
        }
      ],
      "source": [
        "# Define the parameter space for the search\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9, 11, 13],    # Different values for K\n",
        "    'metric': ['euclidean', 'manhattan'],  # The two implemented metrics\n",
        "    'weighted': [True, False]               # Both weighted and unweighted modes\n",
        "}\n",
        "\n",
        "# Define the Cross-Validation strategy (5-Folds)\n",
        "cv_strategy = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Create the GridSearchCV tool using our custom model\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=MyKNNClassifier(), \n",
        "    param_grid=param_grid,\n",
        "    cv=cv_strategy,\n",
        "    scoring='accuracy',  # Our evaluation metric is Accuracy\n",
        "    n_jobs=-1            # Use all CPU cores for faster processing\n",
        ")\n",
        "\n",
        "print(\"Running Grid Search to find the best hyperparameters...\")\n",
        "# Run the search only on the training data\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Search completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best score (Accuracy) in Cross-Validation: 0.9735\n",
            "Best configuration (parameters) found: {'metric': 'euclidean', 'n_neighbors': 11, 'weighted': True}\n"
          ]
        }
      ],
      "source": [
        "# Display search results\n",
        "print(f\"Best score (Accuracy) in Cross-Validation: {grid_search.best_score_:.4f}\")\n",
        "print(f\"Best configuration (parameters) found: {grid_search.best_params_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Final Evaluation and Reporting Results\n",
        "\n",
        "Now that `GridSearchCV` has found the best model (with the best parameters) and stored it in `grid_search.best_estimator_`, we use this final model to make predictions on the **test data** (`X_test`). This data has not been seen during the training and optimization process, making our final evaluation valid.\n",
        "\n",
        "We report the results using `classification_report`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Final Classification Report on the Test Set ---\n",
            "Model Configuration: {'metric': 'euclidean', 'n_neighbors': 11, 'weighted': True}\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        12\n",
            "  versicolor       0.93      1.00      0.96        13\n",
            "   virginica       1.00      0.92      0.96        13\n",
            "\n",
            "    accuracy                           0.97        38\n",
            "   macro avg       0.98      0.97      0.97        38\n",
            "weighted avg       0.98      0.97      0.97        38\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Extract the best model found and trained by GridSearch\n",
        "best_knn_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = best_knn_model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate and print the final report\n",
        "print(\"--- Final Classification Report on the Test Set ---\")\n",
        "print(f\"Model Configuration: {grid_search.best_params_}\\n\")\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final Analysis\n",
        "\n",
        "The report above shows the final performance of our optimized model on unseen data.\n",
        "\n",
        "-   **Precision:** Out of all samples the model predicted as a specific class, what percentage were actually correct?\n",
        "-   **Recall:** Out of all actual samples of a specific class, what fraction did the model correctly identify?\n",
        "-   **F1-Score:** The harmonic mean of Precision and Recall.\n",
        "-   **Accuracy:** The percentage of total correct predictions.\n",
        "\n",
        "As observed, our `MyKNNClassifier` implementation using NumPy achieved very good results (usually over 95% accuracy) on the Iris dataset, and the parameter optimization process was performed correctly."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
